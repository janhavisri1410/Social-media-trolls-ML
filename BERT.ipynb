{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from torchnlp.datasets import imdb_dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datapoints:  12514\n",
      "Positive labels:  4541\n",
      "Negative labels:  7973\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I so much want this election cycle to be over ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@HillaryClinton One small measurement on our e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wahrheit ist die Krücke der Verlierer https://...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinton: I didn't think pneumonia would be a b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a #SaturdayMorning poem:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I so much want this election cycle to be over ...      0\n",
       "1  @HillaryClinton One small measurement on our e...      0\n",
       "2  Wahrheit ist die Krücke der Verlierer https://...      0\n",
       "3  Clinton: I didn't think pneumonia would be a b...      0\n",
       "4                           a #SaturdayMorning poem:      0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = open('Dataset/Positive_tweets(10000).csv').read()\n",
    "npos = 0\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(pos.split(\"\\n\")):\n",
    "    content = line.split(',')\n",
    "    if len(content) < 4:\n",
    "    \tcontinue;\n",
    "    if content[4] != \"English\":\n",
    "    \tcontinue;\n",
    "    labels.append(1)\n",
    "    texts.append(content[2])\n",
    "    npos += 1\n",
    "\n",
    "# load negative labels (random tweets)\n",
    "neg = open('Dataset/Negative_tweets(10000).txt').read()\n",
    "nneg = 0\n",
    "for i, line in enumerate(neg.split(\"\\n\")):\n",
    "    labels.append(0)\n",
    "    texts.append(line)\n",
    "    nneg += 1\n",
    "\n",
    "texts, labels = shuffle(texts, labels)\n",
    "\n",
    "print('Total number of datapoints: ', len(labels))\n",
    "print('Positive labels: ', npos)\n",
    "print('Negative labels: ', nneg)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['text'] = texts\n",
    "df['label'] = labels\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(labels).reshape(-1,1)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if npos < nneg :\n",
    "#     df_bot = df[df['label'] == 1] \n",
    "#     df_normal = df[df['label'] == 0] \n",
    "#     df_normal = df_normal.sample(n=len(df_bot))\n",
    "#     df = df_bot.append(df_normal)\n",
    "#     df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
    "# else :\n",
    "#     df_bot = df[df['label'] == 1] \n",
    "#     df_normal = df[df['label'] == 0] \n",
    "#     df_bot = df_bot.sample(n=len(df_normal))\n",
    "#     df = df_normal.append(df_bot)\n",
    "#     df = df.sample(frac=1, random_state = 24).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10011\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = df.head(len(df)*0.8)\n",
    "# test_data = df.tail(len(df) - len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = [{'text': text, 'label': type_data } for text in list(train_data['text']) for type_data in list(train_data['label'])]\n",
    "# test_data = [{'text': text, 'label': type_data } for text in list(test_data['text']) for type_data in list(test_data['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['label']), train_data)))\n",
    "# test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['label']), test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we truncate the input strings to 128 characters. The maximum length BERT can handle is 512, but in the interest of computational time we will work with 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:127], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:127], test_texts))\n",
    "\n",
    "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=128, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=128, truncating=\"post\", padding=\"post\", dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_labels) == 1\n",
    "test_y = np.array(test_labels) == 1\n",
    "test_y = test_y.astype(int)\n",
    "train_y = train_y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/10011.0 loss: 0.6819517016410828 \n",
      "Epoch:  1\n",
      "100/10011.0 loss: 0.6793890368230272 \n",
      "Epoch:  1\n",
      "200/10011.0 loss: 0.6601348431104451 \n",
      "Epoch:  1\n",
      "300/10011.0 loss: 0.6223950892488822 \n",
      "Epoch:  1\n",
      "400/10011.0 loss: 0.5623385044664814 \n",
      "Epoch:  1\n",
      "500/10011.0 loss: 0.5087928249391015 \n",
      "Epoch:  1\n",
      "600/10011.0 loss: 0.4671702995251697 \n",
      "Epoch:  1\n",
      "700/10011.0 loss: 0.4403937614061863 \n",
      "Epoch:  1\n",
      "800/10011.0 loss: 0.41318554476777863 \n",
      "Epoch:  1\n",
      "900/10011.0 loss: 0.39412806693311536 \n",
      "Epoch:  1\n",
      "1000/10011.0 loss: 0.3776413478072617 \n",
      "Epoch:  1\n",
      "1100/10011.0 loss: 0.36157135857940803 \n",
      "Epoch:  1\n",
      "1200/10011.0 loss: 0.34727883566912265 \n",
      "Epoch:  1\n",
      "1300/10011.0 loss: 0.3363540981274147 \n",
      "Epoch:  1\n",
      "1400/10011.0 loss: 0.32573169580714606 \n",
      "Epoch:  1\n",
      "1500/10011.0 loss: 0.3132142810303616 \n",
      "Epoch:  1\n",
      "1600/10011.0 loss: 0.3042880925632022 \n",
      "Epoch:  1\n",
      "1700/10011.0 loss: 0.2964948245878827 \n",
      "Epoch:  1\n",
      "1800/10011.0 loss: 0.28933278705329035 \n",
      "Epoch:  1\n",
      "1900/10011.0 loss: 0.2796076552438766 \n",
      "Epoch:  1\n",
      "2000/10011.0 loss: 0.27361460962378864 \n",
      "Epoch:  1\n",
      "2100/10011.0 loss: 0.2667555619799831 \n",
      "Epoch:  1\n",
      "2200/10011.0 loss: 0.2619640024472832 \n",
      "Epoch:  1\n",
      "2300/10011.0 loss: 0.2588585857237191 \n",
      "Epoch:  1\n",
      "2400/10011.0 loss: 0.25397346586940106 \n",
      "Epoch:  1\n",
      "2500/10011.0 loss: 0.25058817562953634 \n",
      "Epoch:  1\n",
      "2600/10011.0 loss: 0.25070514343643147 \n",
      "Epoch:  1\n",
      "2700/10011.0 loss: 0.24729661343316486 \n",
      "Epoch:  1\n",
      "2800/10011.0 loss: 0.24418844517402855 \n",
      "Epoch:  1\n",
      "2900/10011.0 loss: 0.24076831560313539 \n",
      "Epoch:  1\n",
      "3000/10011.0 loss: 0.23833553959429016 \n",
      "Epoch:  1\n",
      "3100/10011.0 loss: 0.23493193683143307 \n",
      "Epoch:  1\n",
      "3200/10011.0 loss: 0.23137697725074108 \n",
      "Epoch:  1\n",
      "3300/10011.0 loss: 0.23017398182300455 \n",
      "Epoch:  1\n",
      "3400/10011.0 loss: 0.22683803472608782 \n",
      "Epoch:  1\n",
      "3500/10011.0 loss: 0.22283066687953834 \n",
      "Epoch:  1\n",
      "3600/10011.0 loss: 0.22020069463346625 \n",
      "Epoch:  1\n",
      "3700/10011.0 loss: 0.21922753842648696 \n",
      "Epoch:  1\n",
      "3800/10011.0 loss: 0.2189227457921996 \n",
      "Epoch:  1\n",
      "3900/10011.0 loss: 0.21694041870776024 \n",
      "Epoch:  1\n",
      "4000/10011.0 loss: 0.21425544731009363 \n",
      "Epoch:  1\n",
      "4100/10011.0 loss: 0.2148309098571399 \n",
      "Epoch:  1\n",
      "4200/10011.0 loss: 0.21340467829212467 \n",
      "Epoch:  1\n",
      "4300/10011.0 loss: 0.2111784421808072 \n",
      "Epoch:  1\n",
      "4400/10011.0 loss: 0.20946080772582018 \n",
      "Epoch:  1\n",
      "4500/10011.0 loss: 0.20684090646435302 \n",
      "Epoch:  1\n",
      "4600/10011.0 loss: 0.20331834751923974 \n",
      "Epoch:  1\n",
      "4700/10011.0 loss: 0.20157983492690593 \n",
      "Epoch:  1\n",
      "4800/10011.0 loss: 0.19999751172609856 \n",
      "Epoch:  1\n",
      "4900/10011.0 loss: 0.19715227101956279 \n",
      "Epoch:  1\n",
      "5000/10011.0 loss: 0.19450085967540043 \n",
      "Epoch:  1\n",
      "5100/10011.0 loss: 0.19348604399322006 \n",
      "Epoch:  1\n",
      "5200/10011.0 loss: 0.19508026591304262 \n",
      "Epoch:  1\n",
      "5300/10011.0 loss: 0.19305335134599858 \n",
      "Epoch:  1\n",
      "5400/10011.0 loss: 0.19239918528665448 \n",
      "Epoch:  1\n",
      "5500/10011.0 loss: 0.19132997996966633 \n",
      "Epoch:  1\n",
      "5600/10011.0 loss: 0.18973046883937703 \n",
      "Epoch:  1\n",
      "5700/10011.0 loss: 0.18697522282301451 \n",
      "Epoch:  1\n",
      "5800/10011.0 loss: 0.18539586925665275 \n",
      "Epoch:  1\n",
      "5900/10011.0 loss: 0.18450415056472502 \n",
      "Epoch:  1\n",
      "6000/10011.0 loss: 0.1822100719930522 \n",
      "Epoch:  1\n",
      "6100/10011.0 loss: 0.18112089057385658 \n",
      "Epoch:  1\n",
      "6200/10011.0 loss: 0.1795757278160055 \n",
      "Epoch:  1\n",
      "6300/10011.0 loss: 0.17741028519004107 \n",
      "Epoch:  1\n",
      "6400/10011.0 loss: 0.17776361189253304 \n",
      "Epoch:  1\n",
      "6500/10011.0 loss: 0.1769311897794879 \n",
      "Epoch:  1\n",
      "6600/10011.0 loss: 0.17676371780791741 \n",
      "Epoch:  1\n",
      "6700/10011.0 loss: 0.17684814066381105 \n",
      "Epoch:  1\n",
      "6800/10011.0 loss: 0.17698535877766755 \n",
      "Epoch:  1\n",
      "6900/10011.0 loss: 0.17576618720211198 \n",
      "Epoch:  1\n",
      "7000/10011.0 loss: 0.17394073589784348 \n",
      "Epoch:  1\n",
      "7100/10011.0 loss: 0.17226996452574758 \n",
      "Epoch:  1\n",
      "7200/10011.0 loss: 0.1709569048946999 \n",
      "Epoch:  1\n",
      "7300/10011.0 loss: 0.16916571405150252 \n",
      "Epoch:  1\n",
      "7400/10011.0 loss: 0.16781102686811128 \n",
      "Epoch:  1\n",
      "7500/10011.0 loss: 0.16646915039988977 \n",
      "Epoch:  1\n",
      "7600/10011.0 loss: 0.16493255948650798 \n",
      "Epoch:  1\n",
      "7700/10011.0 loss: 0.1640830966563567 \n",
      "Epoch:  1\n",
      "7800/10011.0 loss: 0.16385378483888213 \n",
      "Epoch:  1\n",
      "7900/10011.0 loss: 0.16226221132808263 \n",
      "Epoch:  1\n",
      "8000/10011.0 loss: 0.16075552777423752 \n",
      "Epoch:  1\n",
      "8100/10011.0 loss: 0.1597544181219203 \n",
      "Epoch:  1\n",
      "8200/10011.0 loss: 0.1601471711928076 \n",
      "Epoch:  1\n",
      "8300/10011.0 loss: 0.15926031929045537 \n",
      "Epoch:  1\n",
      "8400/10011.0 loss: 0.15850944208497164 \n",
      "Epoch:  1\n",
      "8500/10011.0 loss: 0.15802626319116433 \n",
      "Epoch:  1\n",
      "8600/10011.0 loss: 0.1575409785669025 \n",
      "Epoch:  1\n",
      "8700/10011.0 loss: 0.1561680893787172 \n",
      "Epoch:  1\n",
      "8800/10011.0 loss: 0.15640049926009536 \n",
      "Epoch:  1\n",
      "8900/10011.0 loss: 0.1559716031643422 \n",
      "Epoch:  1\n",
      "9000/10011.0 loss: 0.15469387170033827 \n",
      "Epoch:  1\n",
      "9100/10011.0 loss: 0.1537390899868618 \n",
      "Epoch:  1\n",
      "9200/10011.0 loss: 0.1533786660973187 \n",
      "Epoch:  1\n",
      "9300/10011.0 loss: 0.15266609660324454 \n",
      "Epoch:  1\n",
      "9400/10011.0 loss: 0.1524022589345532 \n",
      "Epoch:  1\n",
      "9500/10011.0 loss: 0.1517530404230608 \n",
      "Epoch:  1\n",
      "9600/10011.0 loss: 0.15042243309276046 \n",
      "Epoch:  1\n",
      "9700/10011.0 loss: 0.14949844351735125 \n",
      "Epoch:  1\n",
      "9800/10011.0 loss: 0.14882790740764568 \n",
      "Epoch:  1\n",
      "9900/10011.0 loss: 0.148233182228155 \n",
      "Epoch:  1\n",
      "10000/10011.0 loss: 0.14832255575721534 \n"
     ]
    }
   ],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('Epoch: ', epoch_num + 1)\n",
    "        # print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
    "        if step_num % 100 == 0:\n",
    "            print('Epoch: ', epoch_num + 1)\n",
    "            print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_texts) / BATCH_SIZE, train_loss / (step_num + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1607\n",
      "           1       0.97      0.93      0.95       896\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2503\n",
      "   macro avg       0.97      0.96      0.96      2503\n",
      "weighted avg       0.97      0.97      0.97      2503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9664402716739912\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "7bf5ba559c6092108d1c79914ad4f810e25cf03a64012019e402d699b8cb2028"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
